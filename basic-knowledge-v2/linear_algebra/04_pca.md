
# Principal Component Analysis

主成分分析 (PCA) 是一个非常大的话题, 其在 机器学习 和 统计学 中都有非常重要的应用。本文仅仅说明其在 机器学习 中的应用, 在 统计学 中的应用后续再说。

预备知识: 正交矩阵。对于 方阵 $A$ 来说, 如果其所有列向量两两正交, 并且模长为 1, (也就是说列向量构成空间中的一组 正交基) 那么其就是 **正交矩阵**。对于正交矩阵来说, 其 $A^{-1} = A^T$, 所有行向量的模长也是 1, 并且两两正交。用更数学化的方式表示是: $A A^T = A^T A = I$。

预备知识: 投影。对于高维到低维的投影, 就是在高维空间中的点映射到一个低维子空间中。如果矩阵 $A$ 的行向量构成低维子空间的一组标准正交基, 那么 $\vec{x}$ 和矩阵 $A$ 左乘的结果就是 $\vec{x}$ 投影到标准正交基中的坐标 (从 **行视角** 理解), 记作 $\vec{x}_p$。同时, $\vec{x}_p$ 左乘矩阵 $A^T$ 就是将 $\vec{x}_p$ 转换回原本高维的坐标系中 (从 **列视角** 理解), 记作 $\vec{x}_o$。注意, $\vec{x}_o$ 和 $\vec{x}$ 是不相等的。

PCA 可以用于降维, 即通过 **投影** 的方式将高维向量转换成低维向量, 并保证投影后向量的 **方差** 值最大。(很容易理解, 不同的投影方式, 得到点的方差值是不一致的)

下面我们一步一步来看 PCA 是怎么操作的。

## 基本步骤

对于一维数据来说, 其方差就是: 每一个数字和平均值之间距离 (偏差) 平方的平均值, 用公式表示如下:

$$
var = \frac{1}{n} \sum_{i=1}^n (a^{(i)} - \overline{a})^2 \tag{1}
$$

那么, 对于多维数据, 或者说向量呢? 我们可以理解为每一个维度上的方差值之和。设 $\overline{z}$ 为平均向量, 即其元素值是每一个维度上数据的平均值, 那么:

$$
var = \frac{1}{n} \sum_{i=1}^n ||\vec{z}^{(i)} - \overline{z}||^2 \tag{2}
$$

**两个向量相减** 等于 对应坐标值相减, **向量模长的平方** 等于 向量中每一个元素值的平方再求和。

为了简化公式 $(2)$, 我们需要对数据集中 $z$ 向量的每一个维度进行 **去均值化** (demean) 的操作。这样的另一个好处是, 无论这组数据投影到哪一个向量上, 其均值都是 0。公式 $(2)$ 可以变成:

$$
var = \frac{1}{n} \sum_{i=1}^n ||\vec{z}^{(i)}||^2 \tag{3}
$$

首先, 我们要做的事情是将所有的点投影到一个单位向量 $\vec{w}_1$ 上。沿用上面的标记, $\vec{x}_0$ 是原始数据中的点, $\vec{x}_{w1}$ 是 $\vec{x}_0$ 在 $\vec{w}_1$ 上的投影向量, 其和 $\vec{x}_0$ 维度相同。那么, 此时的目标函数就是:

$$
\begin{align*}
object &= \frac{1}{n} \sum_{i=1}^n ||\vec{x}^{(i)}_{w1}||^2 \\
       &= \frac{1}{n} \sum_{i=1}^n (\vec{x}_0^{(i)} \cdot \vec{w}_1)^2
\end{align*}
\tag{4}
$$

我们希望公式 $(4)$ 中的目标函数值最大, 那么就可以使用 **梯度上升法** 来解决。我们将求解出来的 $\vec{w}_1$ 称为 **第一主成分**。

接下来, 我们怎么求后面的主成分呢? 首先, 将 $\vec{x}_0 - \vec{x}_{w1}$, 得到的向量 $\vec{x}_1$。由 **投影** 的性质可知, $\vec{x}_1$ 一定和 $\vec{w}_1$ 是相互垂直的。接下来要做的事情和之前一样, 寻找一个单位向量 $\vec{w}_2$, 然后套用公式 $(4)$ 即可。

## 应用

这里介绍 PCA 的两个应用: 去噪 和 特征脸。

**去噪** 的意思是说, 将数据从高维映射到低维, 然后再映射回来的过程是有信息损失的。如果处理得当, 损失的信息很可能是 **噪声**。当然, 这只是一种假设, 具体问题具体分析。

**特征脸** 是针对人脸数据集来说的。更一般地, 可以说 特征样本。我们找到的每一个 主成分, 实际上和样本在一个向量空间中。那么, 我们可以将 主成分 当作样本, 这样的样本称为 特征样本。如果去可视化 特征脸, 会发现越靠前的 主成分 包含的信息越多, 图片看起来越模糊; 中间的 主成分 包含的信息刚刚好, 图片中的人脸最为清晰; 靠后的 主成分 包含的信息可能都构不成一张完成的图, 图片中什么都看不出来。

## 总结

这里只是简单了解一下 PCA 算法, 加强对投影相关知识的理解。之后会配合统计学重新了解 PCA, 进而了解 因子分析 算法。
