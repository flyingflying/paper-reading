
# 奇异值分解

## 方阵对角化

定义: 如果方阵 $A$ 和 $B$ 满足 $B = P^{-1} A P$, 则称方阵 $A$ 和 $B$ 是相似矩阵。

那么两个方阵 "相似" 是什么意思呢? 如何理解 "相似" 呢? 观察定义, 由于方阵 $P$ 可逆, 其列向量之间一定是线性无关的。我们将方阵 $P$ 的列向量看作是坐标系, 则方阵 $A$ 和 $B$ 是不同坐标系下的同一 **变换**, 其中方阵 $B$ 在 $P$ 列向量构成的坐标系中, $A$ 在标准坐标系 $\epsilon$ 中。解释如下:

> 假设 $\vec{x}$ 在 $P$ 坐标系中, 记作 $[\vec{x}]_p$, 那么 $B [\vec{x}]_p$ 可以理解为将 $\vec{x}$ 在 $P$ 坐标系中变换一下位置, 其依然在 $P$ 坐标系中。
>
> $P^{-1} A P \cdot [\vec{x}]_p$ 计算过程可以这样理解: $\vec{x}$ 左乘 $P$ 方阵后转换到 标准坐标系 中; 再左乘 $A$ 方阵进行变换, 改变坐标位置, 依然再标准坐标系中; 最后再左乘 $P^{-1}$ 转换回 $P$ 坐标系中。
>
> 两者相等则说明 $A$ 和 $B$ 是同一 **变换**, 只是在不同的坐标系中表示不同。

我们将定义中的公式转换一下, 就可以得到:

$$
A = P B P^{-1} \tag{1}
$$

其中, $A$ 方阵在标准坐标系中的 **变换**, $B$ 方阵在 $P$ 坐标系中的 **变换**。如果某些 **变换** 在标准坐标系下很难计算, 而在 $P$ 坐标系下很容易计算, 那么这样的转换就很有意义了。

接下来需要考虑的事情是, 什么样子的方阵容易计算呢? 答案是 **对角矩阵**。如果 $D$ 是一个 $n$ 阶对角方阵, 其 主对角线 上的元素是 $\lambda$, 非主对角线上的元素是都是 0, 那么此时方阵和向量乘法可以表示为:

$$
D \cdot \vec{x} =
\begin{bmatrix}
    \lambda_1 & 0 & \cdots & 0 \\
    0 & \lambda_2 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & \lambda_n
\end{bmatrix} \cdot
\begin{bmatrix}
    x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix} =
\begin{bmatrix}
    \lambda_1 x_1 \\ \lambda_2 x_2 \\ \vdots \\ \lambda_n x_n
\end{bmatrix}
\tag{2}
$$

接下来的问题就变成如何求解 对角方阵 $D$ 了。我们将公式 $(1)$ 左右两边同乘以 $P$, 然后将 $B=D$ 代入, 可以得到: $AP = PD$。接下来, 我们设方阵 $P$ 的列向量是 $\vec{u}$, 根据 矩阵和矩阵乘法的方式, 可以得到:

$$
AP = \begin{pmatrix}
    A\vec{u_1} & A\vec{u_2} & \cdots & A\vec{u_n}
\end{pmatrix} \tag{3}
$$

$$
PD = \begin{pmatrix}
    \lambda_1 \vec{u_1} & \lambda_2 \vec{u_2} & \cdots & \lambda_n \vec{u_n}
\end{pmatrix} \tag{4}
$$

由此可以得到:

$$
A \vec{u} = \lambda \vec{u} \tag{5}
$$

我们将 $\lambda$ 和 $\vec{u}$ 称为方阵 $A$ 的 **特征值** 和 **特征向量**。而公式 $(5)$ 就是它们在教科书中的定义。简单来说, **特征向量** 构成了一个新的向量空间, 如果将方阵 $A$ 转换到这个向量空间中, 就会变成对角方阵, 对角线上的元素值就是 **特征值**。

$\vec{u}$ 肯定不能是零向量。从公式 $(5)$ 可以看出, 特征向量的特点是: $\vec{u}$ 经过 $A$ 变换后和 $\vec{u}$ 在同一条直线上。我们将公式 $(5)$ 变形, 可以得到:

$$
(A - \lambda I) \vec{u} = \vec{0} \tag{6}
$$

对于公式 $(6)$ 中的方程式, $\vec{u} = \vec{0}$ 是方程的一个解。我们现在不希望这个方程式有唯一解, 也就意味着 $A - \lambda I$ 方阵不是可逆矩阵。换言之, 如果将方阵 $A$ 主对角线上的元素减去特征值, 那么某些行向量之间会产生 线性相关 的关系。

如何求解特征值这里就不说了, 涉及到行列式相关的知识。我们将整个过程称为 **矩阵的对角化**。需要注意的是:

+ 如果 $A$ 方阵有 $n$ 个线性无关的特征向量, 则 $A$ 方阵可以被对角化
+ $A$ 方阵是否可逆和其能不能对角化是没有关系的
+ $A$ 方阵不可逆, 则其有一个特征值是 0

公式 $(1)$ 也可以理解为方阵的分解, 即将一个方阵分解成三个方阵的乘积。

## SVD 分解的前置知识

SVD 分解的原理是基于 **对称矩阵** 和 **标准正交矩阵** 的性质的, 那么它们究竟有什么样的性质呢?

如果方阵 $A$ 是 **对称矩阵**, 即 $A = A^T$, 其充分必要条件是: $A$ 一定有 $n$ 个线性无关的特征向量, 并且这些特征向量两两垂直。换言之, 如果方阵 $A$ 是对称矩阵, 那么其一定可以被对角化, 同时 $P$ 坐标系是正交坐标系。可以说, 对称矩阵很完美!

如果方阵 $A$ 的所有列向量模长为 1, 且两两之间相互垂直, 那么就是 **标准正交矩阵**。其行向量的模长也是 1, 且也是两两之间相互垂直。并且, 最重要的: $A^{-1} = A^T$。

上面的结论都是针对方阵而言的, 那么对于非方阵应该怎么去研究呢? 答案是构造方阵。

## SVD 分解

对于任意矩阵 $A$ 来说, 其 $A^T A$ 一定是方阵, 且一定是 **对称矩阵**。如果 $A$ 的维度是 $m \times n$, 那么 $A^T A$ 的维度是 $n \times n$。

根据 **对称矩阵** 的性质, $A^T A$ 可以被正交对角化。我们设 $\lambda$ 为 $A^T A$ 矩阵的特征值, $\vec{v}$ 为 $A^T A$ 矩阵的特征向量。那么 $\vec{v}$ 之间是两两垂直的。

我们 $A^T A$ 所有的特征向量进行归一化操作, 变成单位向量。然后拼在一起, 构成矩阵 $V$。这个矩阵是 **标准正交矩阵**, 也就是说 $V^{-1} = V^T$。

接下来, 我们分析 $A \vec{v}$ 这个 $m$ 维的向量的特性, 可以得到如下的结论:

+ $A \vec{v}$ 是矩阵 $A$ 列空间中的一组正交基
+ $A \vec{v}$ 的模长是 $\sqrt{\lambda}$

具体的证明过程就不写了, 你可以根据 对称矩阵 的性质得到。需要注意的是: $\lambda$ 的值一定是大于或者等于 0 的。如果 $\lambda$ 的值等于 0, 那么 $A \vec{v}$ 就是零向量, 此时就不能构成空间中的一组基了, 更具体地说:

如果矩阵 $A^T A$ 有 $r$ 个非零的特征值 $\lambda$。这些特征值对应的 $A \vec{v}$ 构成 $A$ 矩阵列空间中的一组基。剩下的 $m - r$ 维度可以取任意的向量, 但是我们为了统一, 使用 Gram-Schmidt 的方式补全剩下的向量, 让所有的向量两两垂直。

我们将 $\sqrt{\lambda}$ 称为 **奇异值**, 用 $\sigma$ 表示。由这些奇异值构成的对角线矩阵记作 $\Sigma$, 其和矩阵 $A$ 的维度是一致的, 都是 $m \times n$。

$A \vec{v}$ 不是单位向量, 我们就除以其模长, 让它们变成单位向量, 即 $\vec{u} = \frac{A \vec{v}}{\sigma}$。由 $\vec{u}$ 构成的矩阵 $U$ 也是 **标准正交矩阵**。

熟悉矩阵乘法的朋友应该会发现, 此时 $U \Sigma$ 就是由 $A \vec{v}$ 向量构成的矩阵, 那不就是 $AV$ 吗? 接下来:

$$
\begin{align*}
    AV &= U \Sigma \\
    A &= U \Sigma V^{-1} \\
    A &= U \Sigma V^T
\end{align*}
$$

我们将上述过程称为 奇异值分解 (SVD)。

接下来我们计算 $U \Sigma V^T$ 的值, 可以得到:

$$
\begin{align*}
U \Sigma V^T &=
\begin{bmatrix}
    \vec{u_1} &
    \vec{u_2} &
    \cdots &
    \vec{u_r} &
    \vec{u_{r+1}} &
    \cdots &
    \vec{u_m}
\end{bmatrix} \cdot
\begin{bmatrix}
    \sigma_1 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
    0 & \sigma_2 & \cdots & 0 & 0 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & \sigma_r & 0 & \cdots & 0 \\
    0 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
\end{bmatrix} \cdot
\begin{bmatrix}
    \vec{v_1} \\
    \vec{v_2} \\
    \vdots \\
    \vec{v_r} \\
    \vec{v_{r+1}} \\
    \vdots \\
    \vec{v_n}
\end{bmatrix} \\ &=
\begin{bmatrix}
    \sigma_1 \vec{u_1} &
    \sigma_2 \vec{u_2} &
    \cdots &
    \sigma_r \vec{u_r} &
    0 &
    \cdots &
    0
\end{bmatrix} \cdot
\begin{bmatrix}
    \vec{v_1} \\
    \vec{v_2} \\
    \vdots \\
    \vec{v_r} \\
    \vec{v_{r+1}} \\
    \vdots \\
    \vec{v_n}
\end{bmatrix} \\ &=
\sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T + \cdots + \sigma_r u_r v_r^T
\end{align*}
\tag{7}
$$

上面的公式 $(7)$ 非常重要, 一定要理解, 其揭示了 SVD 分解的意义。第一步到第二步是矩阵乘法的 矩阵-列向量 视角, 第二步到第三步是矩阵乘法的 列向量-行向量 视角。

实际上, $A^T A$, $A A^T$ 和 $A$ 三个矩阵的秩是相等的。如果为 $r$, 那么就意味着 $A$ 有 $r$ 个非零的奇异值。从公式 $(7)$ 可以看出, 我们完全可以不用关心零奇异值的情况, 都是可以去掉的, 对 $A$ 矩阵没有任何的影响。那么 $U$ 矩阵可以简化为 $m \times r$ 的矩阵, $V$ 矩阵可以简化为 $r \times n$, $\Sigma$ 是一个 $r \times r$ 的对角矩阵。我们需要存储的元素个数就是 $(m + n + 1) \times r$。原本矩阵 $A$ 的元素个数是 $m \times n$。如果矩阵 $A$ 是非常低秩的矩阵, 那么就可以用这种方式实现压缩。

不仅如此, 在实际上进行 SVD 分解时, 我们会对奇异值按照从小到大的顺序排列。从公式 $(7)$ 可以看出, 越大的奇异值对矩阵 $A$ 的重要性越高。我们可以将靠近 0 的奇异值都去掉, 从而实现进一步的压缩, 只是现在是有损压缩了。

## LSA 与 SVD

对于 TF 形式的文本向量, 词表中的每一个词就是特征, 特征值就是词语在文本中出现的频率。将多个 **文本 TF 向量** 拼接在一起, 构成 `[n_docs, vocab_size]` 样式的矩阵。我们将这样的矩阵记作 $A$。

熟悉 NLP 的朋友应该都知道, $A$ 是一个稀疏矩阵, 也就是一个低秩矩阵。我们可以认为 **文本 TF 向量** 在一个 `vocab_size` 维度的空间中, 这个空间记作 **词语空间**。同时, **文本 TF 向量** 也存在于 **词语空间** 中的一个子空间中, 我们将这个子空间称为 **主题空间**。

我们假设 **词语空间** 和 **主题空间** 满足某种线性关系, 那么现在要做的事情是: 将 **文本 TF 向量** 从 **词语空间** 投影到 **主题空间** 中, 具体做法如下:

1. 在 **主题空间** 中寻找一组标准正交基, 按照列向量方式排列构成矩阵 $V$, 其维度是 `[vocab_size, topic_size]`
2. **文本 TF 向量** 和 $V^T$ 进行左乘, 相当于投影到 **主题空间** 中, 记作 **文本向量**
3. **文本向量** 和 $V$ 进行左乘, 相当于投影回原本的 **词语空间** 中

对上述过程有疑问, 建议复习 **投影** 相关的知识。其中, 第三步用公式表示是:

$$ A = A^{\prime} \cdot V^T \tag{8}$$

$A^{\prime}$ 的行向量就是 **文本向量**, 其维度是 `[n_docs, topic_size]`。(使用 $V^T$ 的原因是左乘变右乘)

上述过程不正好是矩阵的分解吗? 我们将矩阵 $A$ 分解为 $A^{\prime} V^T$ 的形式。那么, 我们可以用 **矩阵分解** 的方式来求解 $A^{\prime}$。换言之, **矩阵分解** 的过程直接帮助我们找到了 **词语空间** 和 **主题空间** 的线性关系。

在这种情况下, `n_docs` 也被当作是一个维度。那么, 我们可以认为 矩阵 $A$ 的行空间就是 **词语空间**, 矩阵 $A$ 的 **秩** 就是 **主题空间** 的维度。

那么上面所说的东西和 SVD 有什么关系呢? 我们将维度是 `[n_docs, vocab_size]` 的矩阵 $A$ 进行 SVD 分解, 然后只保留非零奇异值, 可以得到三个矩阵:

+ $U$: `[n_docs, topic_size]` 维度的矩阵, 两两列向量之间相互垂直
+ $\Sigma$: `[topic_size, topic_size]` 维度的对角矩阵
+ $V^T$: `[topic_size, vocab_size]` 维度的矩阵, 两两行向量之间互相垂直

SVD 分解得到的 $V^T$ 矩阵正好满足公式 $(8)$ 中的 $V^T$, 我们将 $U \Sigma$ 矩阵当作 $A^\prime$ 矩阵。这样, 我们就求得了矩阵 $V$。

需要注意的是, $A^{\prime}$ 不是一个方阵, 其列向量之间是相互垂直的, **文本向量** 是其行向量, 由于被截断了, 不是相互垂直的, 概念不要弄混淆了。

当然, 从公式 $(7)$ 中可以知道, `topic_size` 可以是一个超参数, 我们可以将较小的奇异值忽略, 只要 "近似" 即可。

另外, 额外说明一点, 和 LDA 不一样, **文本 TF 向量** 不单单使用词频, 还要经过 tf-idf 算法的转换, 这样的结果会相对好一些。

整个方案最大的问题是: **主题空间** 真的是 **词语空间** 的线性子空间吗? 比方说, 如果 **词语空间** 是一个三维空间, **主题空间** 很可能是三维空间中的一个曲面, 而非平面。但是 SVD 分解直接假设 **主题空间** 是曲面, 肯定是不合理的。

除此之外, $A$ 矩阵可能会非常巨大, 无法正常地进行 SVD 分解。具体可以参考 sklearn 中的 [sklearn.decomposition.TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) 方法。

更一般地, 我们可以将其作为一种通用的 **降维** 方式, 不仅仅用于主题模型。

## 总结

SVD 分解非常地巧妙。是基于大量矩阵的特性。矩阵特性往往是比较难理解的, 主要原因有:

+ 计算不够灵活, 只定义了: 加法和乘法, 减法比较简单, 除法就不存在
+ 大于 三维 的空间无法想象: 比方说, 两个平面在四维空间正交
+ 基础知识不够扎实
