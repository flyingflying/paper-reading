
# 扩散模型

扩散模型最初的构想是 2015 年的论文: [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585)。但真正出名是因为 2020 年的论文: [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239), 简称 DDPM, 翻译一下是 去噪扩散概率模型。2022 年谷歌大佬写了一篇综述, 详细阐述了其中的数学原理: [Understanding Diffusion Models: A Unified Perspective](https://arxiv.org/abs/2208.11970)。

本文主要结合上面的内容, 以及网络上的各种资料, 来初步介绍 扩散模型 的原理。

## 基础知识

这一部分列一些基础的概率知识。如果不懂, 可以当作先当作定理记住, 之后再去搜索相关的知识进行补充。

**知识一**: **独立高斯分布的可加性**

如果随机变量 $X$ 符合 $N(\mu_x, \sigma_x^2)$, 随机变量 $Y$ 符合 $N(\mu_y, \sigma_y^2)$, 且两者之间是相互独立的, 那么: $X + Y$ 这个随机变量符合 $N(\mu_x+\mu_y, \sigma_x^2+\sigma_y^2)$。

需要注意和 高斯混合模型 的区别, 一个是分布叠加, 一个是随机变量叠加。

**知识二**: **重参数技巧** (reparameterization trick)

如果我们希望从 $N(\mu, \sigma^2)$ 中采样数据, 那么我们可以转化为: 先从 $N(0, 1)$ 中采样数据, 再将数据乘以 $\sigma$ 加上 $\mu$。两者是等价的。

这个其实很好理解, 不理解想一想 z-score 化的过程。

## 扩散过程

扩散现象 是 非平衡热力学 研究的内容。如果将一滴墨水滴入一杯清水中, 这滴墨水会慢慢扩散开, 扩散速率也在慢慢变快, 直到最后融为一体。

对于图像来说, 我们可以理解为高维空间中的一个 **点**。这些 **点** 聚集在高维空间的某些区域。如果我们不断地往 图片 中加入 高斯噪声, 改变 **点** 的位置, 那么最终这些点在高维空间中呈现高斯分布。

由于我们是一步一步加入 高斯噪声 的, 那么我们就可以使用 马尔可夫链 来描述问题。

设一共有 $T$ 个时刻, $x_t$ 是每一个时刻加入 高斯噪声 后的图片, $x_0$ 是初始的数据集中的图片, $x_T$ 是最后一个时刻的加入高斯噪音的图片, 也可以认为就是全噪声图片。

设每一个时刻加入高斯噪声为 $\gamma_t$, 其是从均值是 $0$, 方差是 $\beta_t$ 的高斯分布中采样得到的。在扩散现象中, 墨水扩散的区域随着时间的增加而变大, 对应到这里就是 $\beta_t$ 随着时刻的增加而增大。在论文中 $\beta_t$ 也被称为 **扩散率** (diffusion rate)。$\beta_t$ 的取值一般在 $0$ 到 $1$ 之间。

我们设 $\epsilon$ 是从 $N(0, 1)$ 中采样出来的高斯噪声, $\epsilon_{t}$ 表示 $t$ 时刻从 $N(0, 1)$ 中采样得到的噪音。那么 $\gamma_t$ 可以写成 $\sqrt{\beta_t} \cdot \epsilon_t$ 。

最终, 我们希望 $x_T$ 符合标准正态分布, 即 $N(0, 1)$, 那么可以得到:

$$
x_t = \sqrt{1 - \beta_t} \cdot x_{t-1} + \sqrt{\beta_t} \cdot \epsilon_t \tag{1}
$$

那么公式 $(1)$ 是如何得到的呢? 或者说, 如何通过公式 $(1)$ 推导出 $x_T$ 是符合 $N(0, 1)$ 分布的呢? 我看了网上很多的博客, 都没有说明其原因。在 [最初的论文](https://arxiv.org/abs/1503.03585) 中, 也没有给出说明, 只是在第 15 页的 Table App.1. 中给出了类似的公式。我没有深入的了解过 随机过程, 不知道其中有没有答案。这里给出一种看似合理的解释:

> 假定 $x_{t-1}$ 是符合 $N(0, 1)$ 分布的, 那么 $\sqrt{1 - \beta_t} \cdot x_{t-1}$ 符合均值为 0, 方差为 $1 - \beta_t$ 的正态分布。
> 我们已知 $\sqrt{\beta_t} \cdot \epsilon_t$ 符合均值为 0, 方差为 $\beta_t$ 的正态分布; 且 $x_{t-1}$ 和 $\epsilon_t$ 两者间是独立的。
> 根据 **独立高斯分布的可加性**, 那么 $x_t$ 符合均值为 0, 方差为 $1 - \beta_t + \beta_t$ 的分布, 即 $N(0, 1)$ 分布。
> 虽然 $x_{t-1}$ 一开始并不是符合 $N(0, 1)$ 分布的, 但是随着迭代的, 会慢慢符合。

当然, 这只是一种直观上的理解, 并不严谨。至于为什么联想到 随机过程, 是因为想到了 马尔可夫链 的收敛性 (参考 page rank 算法)。

公式 $(1)$ 是通过迭代方式定义的。现在我们将其改成非迭代的方式 (下面的公式 $(3)$):

> 由公式 $(1)$ 可以得到:
>
> $$x_1 = \sqrt{1 - \beta_1} \cdot x_0 + \sqrt{\beta_1} \cdot \epsilon_1 \tag{2.1}$$
> $$x_2 = \sqrt{1 - \beta_2} \cdot x_1 + \sqrt{\beta_2} \cdot \epsilon_2 \tag{2.2}$$
>
> 我们将公式 $(2.1)$ 代入公式 $(2.2)$, 可得:
>
> $$x_2 = \sqrt{1 - \beta_2} \cdot \sqrt{1 - \beta_1} \cdot x_0 + \sqrt{\beta_1(1 - \beta_2)} \cdot \epsilon_1 + \sqrt{\beta_2} \cdot \epsilon_2 \tag{2.3}$$
>
> 由于 $\epsilon_1$ 和 $\epsilon_2$ 是由两次独立的高斯分布采样得到的, 根据 **独立高斯分布的可加性**, 我们可以变成一次高斯分布, 则:
>
> $$x_2 = \sqrt{1 - \beta_2} \cdot \sqrt{1 - \beta_1} \cdot x_0 + \sqrt{\beta_1 - \beta_1 \beta_2 + \beta_2} \cdot \bar{\epsilon}_2 \tag{2.4}$$
>
> 其中 $\bar{\epsilon}_2$ 是从 $N(0, 1)$ 中采样得到的噪声。对公式 $(2.4)$ 进行进一步转换, 可得:
>
> $$x_2 = \sqrt{1 - \beta_2} \cdot \sqrt{1 - \beta_1} \cdot x_0 + \sqrt{1 - (1 - \beta_2)(1 - \beta_1)} \cdot \bar{\epsilon}_2 \tag{2.4}$$
>
> 由此可以推导出更一般的公式:
> $$x_t = \sqrt{1 - \beta_t} \cdots \sqrt{1 - \beta_1} \cdot x_0 + \sqrt{1 - (1 - \beta_t) \cdots (1 - \beta_1)} \cdot \bar{\epsilon}_t \tag{2}$$

为了简化公式 $(2)$, 我们设 $\bar{\alpha}_t = (1 - \beta_t) \cdots (1 - \beta_1) = \prod_{i=1}^{t} (1 - \beta_i)$, 那么可以得到:

$$
x_t = \sqrt{\bar{\alpha}_t} \cdot x_0 + \sqrt{1 - \bar{\alpha}_t} \cdot \bar{\epsilon}_t \tag{3}
$$

在 DDPM 中, $T$ 的取值是 1000, 即有 1000 步。$\beta_t$ 是呈线性变换的。$\beta_1$ 的值是 $10^{-4}$, $\beta_T$ 的值是 $0.02$, 整体上是随着 时刻 的增加而变大。

$\bar{\alpha}_t$ 是小于 0 的数字连乘, 随着 时刻 的增加而变小。在 $T$ 时刻, $\bar{\alpha}_T$ 是趋近于 0 的, 也就是说 $x_T$ 趋近于 $\bar{\epsilon}_t$, 即 $x_T$ 符合 $N(0, 1)$。当然, 这也是一种感性的理解。

总结一下, 扩散过程就是通过公式 $(1)$ 不断往图片中加入 $N(0, \beta)$ 的噪声, 使得最终的分布是 标准正态分布 $N(0, 1)$。

## 逆扩散过程

扩散过程最终的概率分布是 $N(0, 1)$。了解过图片生成的应该能猜到, 我们只需要将整个过程反过来, 就能生成图片了。

换言之, 图片生成的过程就是: 首先从 $N(0, 1)$ 中采样出来一张全噪声图, 然后进行 $T$ 步 **去噪** 操作, 最后得到的图片就是需要生成的图片。整个过程被称为 **逆扩散过程**。

根据公式 $(1)$, 我们可以得到逆扩散过程的公式:

$$
x_{t-1} = \frac{x_t - \sqrt{\beta_t} \cdot \epsilon_t}{\sqrt{1 - \beta_t}} \tag{4}
$$

需要注意的是, $\epsilon_t$ 不能从 $N(0, 1)$ 中采样得到, 需要通过神经网络进行估算。比较形象化的解释是: 全噪图片就是一块大理石, 去噪过程是 **雕刻**。虽然雕刻的方式有很多种, 但是也不能随意雕刻, 否则是变不成雕像的。神经网络的输入是 $x_t$, 输出是 $\epsilon_t$, 然后根据公式 $(4)$ 来计算 $x_{t-1}$。

但是这样会产生一个很大的问题: 采样。如果整个过程需要加 1000 次噪声, 那么我们就需要将 1000 次去噪的结果保存下来, 这样会消耗大量的资源 (时间和空间)。怎么解决这个问题呢? 答案是不使用公式 $(1)$, 而是使用公式 $(3)$ 来生成样本。根据公式 $(3)$, 我们可以直接得到 $x_t$ 的结果, 这样就不需要存储每一步的结果。但是, 此时神经网络预测的不是 $\epsilon_{t}$, 而是 $\bar{\epsilon}_t$ 。

需要特别说明的是: 在扩散过程中, $\epsilon_t$ 和 $\bar{\epsilon}_t$ 可以理解为一个概念, 都是从 $N(0, 1)$ 中采样出来的噪声, 但是在 逆扩散过程 中, 这完全是两个概念。神经网络输出的 $\epsilon_t$ 表示的是从 $x_{t}$ 到 $x_{t-1}$ 之间的噪声, 而 $\bar{\epsilon}$ 表示的是从 $x_{t}$ 到 $x_0$ 之间的噪声。形象化的解释是: $\epsilon_t$ 是雕刻一步的方式; $\bar{\epsilon}_t$ 是雕刻完成的方式。

此时的神经网络输入是 $x_t$ 和 $t$, 输出是 $\bar{\epsilon}_t$。这一段下面的部分是我自己的猜测: 在预测 $\bar{\epsilon}_t$ 时, 神经网络始终是计算 输入图片 $x_t$ 和 生成图片 $x_0$ 之间的噪声, $t$ 可以起到告诉模型 **去噪程度** 的作用, 相当于一个连续型的变量, 此时我们只需要一个神经网络就可以达到比较好的效果。但是在预测 $\epsilon_t$ 时, 神经网络计算的是每一步的噪声, 此时 $t$ 是一个离散型的变量, 我们不能直接输入到模型中。如果想达到比较好的效果, 可能需要 $T$ 个模型。我猜测这是另一个预测 $\bar{\epsilon}_t$, 而不是 $\epsilon_t$ 的原因。

这时候, 你也许会有疑问, 如果用公式 $(3)$ 来生成样本, 我们是不是可以一步直接计算得到 $x_0$ 的值, 不需要一步一步计算呢? 计算公式如下:

$$
x_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \bar{\epsilon}_t}{\sqrt{\bar{\alpha}_t}} \tag{5}
$$

这样做是没问题的, 但是 **效果不好**。我们希望的是 "逐步" 生成图片, 而不是 "一步到位" 式的生成图片。

Diffusion Model 效果好的原因可能也就是在这里。在 GAN 架构中, 我们往往采用 "一步到位" 的方式来生成图片。相关内容计划写另一篇文章来讨论。

也就是说, Diffusion Model 的训练过程使用的是 **非迭代** 的方式, 而推理过程使用的是 **迭代** 的方式。

"逐步" 生成图片的公式是什么呢? 这会在下一部分进行推导说明, 也是 扩散模型 最难的地方, 或者说数学最多的地方。

## 公式推导

在公式 $(1)$ 中, $\epsilon_t$ 符合 $N(0, 1)$, 即标准正态分布。根据 **重参数技巧**, 我们可以得到 $x_t$ 符合均值是 $\sqrt{1 - \beta_t} \cdot x_{t-1}$, 方差是 $\beta_t$ 的正态分布, 即 $N(\sqrt{1 - \beta_t} \cdot x_{t-1}, \beta_t)$ 。这就是 [最初的论文](https://arxiv.org/abs/1503.03585) 中第 15 页的 Table App.1. 中的公式。

正态分布的公式是已知的, $\beta_t$ 是预先设置好的参数, 那么在已知 $x_{t-1}$ 的情况下, 就可以知道 $x_t$ 的概率密度了。即 $p(x_{t}|x_{t-1})$ 是已知的。

同理, 根据公式 $(3)$, $x_t$ 也符合 $N(\sqrt{\bar{\alpha}_t} \cdot x_0, \sqrt{1 - \bar{\alpha}_t})$ 。也就是说 $p(x_t | x_0)$ 是已知的。

逆扩散过程可以转化为: 在已知 $x_t$ 的情况下, 求 $x_{t-1}$ 的概率分布。然后从这个概率分布中采样, 获得 $x_{t-1}$ 。换言之, 就是求 $p(x_{t-1}|x_t)$ 。根据贝叶斯公式, 我们可以知道:

$$
p(x_{t-1}|x_t) = p(x_t|x_{t-1}) \cdot \frac{p(x_{t-1})}{p(x_t)} \tag{6.1}
$$

在公式 $(6.1)$ 中, 我们只知道 $p(x_t|x_{t-1})$, 却不知道 $p(x_{t-1})$ 和 $p(x_t)$ 。接下来, 我们需要一些技巧来解决问题了。如果再加一个条件 $x_0$, 能不能求呢? 根据贝叶斯公式可知:

$$
p(x_{t-1}|x_t, x_0) = p(x_t|x_{t-1}, x_0) \cdot \frac{p(x_{t-1} | x_0)}{p(x_t | x_0)} \tag{6.2}
$$

上面说过了, $p(x_{t-1}|x_0)$ 和 $p(x_t|x_0)$ 和都是已知的。那 $p(x_t | x_{t-1}, x_0)$ 呢? 其和 $p(x_t|x_{t-1})$ 是相等的。观察公式 $(1)$, 我们可以发现, 在已知 $x_{t-1}$ 的情况下, 无论之前的图片是什么样子, $x_t$ 的概率分布都是确定的, 整个过程是符合 [Markov Property](https://en.wikipedia.org/wiki/Markov_property) 的, 因此 $p(x_t | x_{t-1}, x_0) = p(x_t|x_{t-1})$。此时公式 $(6.2)$ 变成:

$$
p(x_{t-1}|x_t, x_0) = p(x_t|x_{t-1}) \cdot \frac{p(x_{t-1} | x_0)}{p(x_t | x_0)} \tag{6}
$$

其中, $x_t | x_{t-1}$ 符合 $N(\sqrt{1 - \beta_t} \cdot x_{t-1}, \beta_t)$ 分布; $x_{t-1} | x_0$ 符合 $N(\sqrt{\bar{\alpha}_{t-1}} \cdot x_0, \sqrt{1 - \bar{\alpha}_{t-1}})$ 分布; $x_t | x_0$ 符合 $N(\sqrt{\bar{\alpha}_t} \cdot x_0, \sqrt{1 - \bar{\alpha}_t})$ 分布。

下面要做一件非常疯狂的事情。将这三个部分用 [正态分布公式](https://en.wikipedia.org/wiki/Normal_distribution) 进行计算化简, 可以得到 $x_{t-1}|x_t, x_0$ 也是符合正态分布的。具体的化简过程参考 [综述](https://arxiv.org/abs/2208.11970) 的第 12 页公式 $(71)$ 到 $(84)$。我们设这个分布的均值是 $\mu_t$, 方差是 $S_t$。化简结果是:

$$
\mu_t = \frac{\sqrt{\bar{\alpha}_{t-1}} \cdot \beta_t \cdot x_0 + \sqrt{1 - \beta_t} \cdot (1 - \bar{\alpha}_{t-1}) \cdot x_t }{1 - \bar{\alpha}_t} \tag{7.1}
$$

$$
S_t = \frac{ 1 - \bar{\alpha}_{t-1} }{ 1 - \bar{\alpha}_{t} } \cdot \beta_t \tag{8}
$$

需要注意的是, 在论文中, 还有两个符号, 本文中没有使用。分别是: $\sigma_t = \sqrt{\beta_t}$ 和 $\alpha_t = 1 - \beta_t$。还有注意 $\alpha_t$ 和 $\bar{\alpha}_t$ 的区别。

由此可以发现, 对于 $x_{t-1}|x_t, x_0$ 这个正态分布来说, 方差 $S$ 是固定的, 不会随着 $x_0$ 和 $x_t$ 的变化而变化, 但是 $\mu$ 会。我们将公式 $(5)$ 代入到公式 $(7.1)$ 中, 化简过程参考 [综述](https://arxiv.org/abs/2208.11970) 的第 15 页公式 $(116)$ 到 $(124)$, 可以得到:

$$
\mu_t = \frac{1}{\sqrt{1 - \beta_t}} \cdot (x_t - \frac{\beta_t}{ \sqrt{1 - \bar{\alpha}_t} } \cdot \bar{\epsilon}_t ) \tag{7}
$$

最终, 我们可以发现, $p(x_{t-1}|x_t, x_0) = p(x_{t-1}|x_t, \bar{\epsilon}_t)$。在已知 $\bar{\epsilon}_t$ 的情况下, 我们就可以由 $x_t$ 推出 $x_{t-1}$ 的概率分布, 逆扩散过程也就可以进行了。

更进一步说, 扩散过程是符合 马尔可夫假设 的, 我们根据 $x_{t-1}$ 就可以知道 $x_t$ 的概率分布了, 不需要额外的条件, 但是 逆扩散过程 则不同, 其不符合 马尔可夫假设, 因为我们不仅需要 $x_t$, 还需要 $\bar{\epsilon}_t$ 才能得到 $x_{t-1}$ 的概率分布。而 $\bar{\epsilon}_t$ 是神经网络预测出来的。

## 总结

从上面的内容可以知道, 逆扩散过程就是: 先从 $N(0, 1)$ 中采样出一张全噪声图片, 作为 $x_T$。然后用神经网络预测 $\bar{\epsilon}_t$, 计算出来 $\mu_t$, 再从 $N(\mu_t, S_t)$ 中采样出 $x_{t-1}$。如此往复, 得到 $x_0$, 就是我们需要的图片了。

需要注意的是, 在实际推理的过程中, 我们并不是从 $N(\mu_t, S_t)$ 中进行采样, 而是从 $N(\mu_t, \beta_t)$ 中进行采样。从公式 $(8)$ 可以知道, $S_t < \beta_t$ 的, 两者之间差了一项 $\frac{ 1 - \bar{\alpha}_{t-1} }{ 1 - \bar{\alpha}_{t} }$。目前还没有特别合理的解释。不过当 $t > 100$ 时, 两者之间的差距并不是很大。

如果熟悉 正态分布 的话, 你可能会想, 逆向过程一定是在 $\mu_t$ 处概率密度最大, 我们完全可以不用采样, 直接使用 $\mu_t$ 就好了。这和文本生成中为什么要进行采样是相似的问题。一般的解释是, 概率最大的位置往往不是我们想要的, 人也不是靠 概率 来画画的, 我们只要取高概率处的内容保证图画的合理即可。这也是一个值得研究的问题。

整个训练过程和推理过程在 [DDPM](https://arxiv.org/abs/2006.11239) 论文第 4 页的 Algorithm 1 和 Algorithm 2。这里用中文表述一下:

用 $\epsilon_\theta$ 表示神经网络, 训练过程是:

1. 从数据集中采样出来 $x_0$
2. 从 1 到 T 的均匀分布中采样出来 $t$
3. 从标准正态分布中采样出来 $\epsilon$
4. 用公式 $(3)$ 计算出来 $x_t$
5. 计算 $\epsilon$ 和 $\epsilon_\theta (x_t, t)$ 之间的 MSE, 作为 loss 值
6. 使用 loss 值反向传播, 更新 $\theta$ 的值
7. 反复执行 1 至 6 步, 直至收敛

推理过程是:

1. 从标准正态分布中采样出 $x_T$
2. t = T 到 1, 进入循环:
   + 使用神经网络估算噪声 $\bar{\epsilon}_t = \epsilon_\theta (x_t, t)$
   + 根据 $\bar{\epsilon}_t$, 使用公式 $(7)$, 计算 $\mu_t$
   + 如果 $t > 1$, $z$ 就是从标准正态分布中采样出来的噪声, 否则就是零
   + 计算 $x_{t-1} = \mu + \sqrt{\beta_t} \cdot z$
3. 返回 $x_0$

在 [综述](https://arxiv.org/abs/2208.11970) 中, 作者通过 ELBO 将 扩散模型 和 极大似然法 联系在一起, 整个训练过程是在最大化 **数据集图片** 概率分布和 **生成图片** 概率分布之间 KL 散度的 **下界**。这样, 扩散模型就可以和 VAE 模型联系在一起了, 有兴趣可以自行阅读。

额外说明一下, 虽然现在说 GAN 被扩散模型打败了, 但是他们属于不同方向上的优化方案, 你甚至可以将扩散模型和 GAN 联合在一起使用。真正和扩散模型属于同类的是 VAE !!!

## References

+ [由浅入深了解Diffusion Model](https://zhuanlan.zhihu.com/p/525106459)
+ [Diffusion Model 背后的数学原理](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/DDPM%20(v7).pdf)
+ [生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪](https://spaces.ac.cn/archives/9164)
+ [54、Probabilistic Diffusion Model概率扩散模型理论与完整PyTorch代码详细解读](https://www.bilibili.com/video/BV1b541197HX)
+ [Diffusion Model：比“GAN"还要牛逼的图像生成模型！公式推导+论文精读，迪哥打你从零详解扩散模型！](https://www.bilibili.com/video/BV1pD4y1179T)
+ [一个视频看懂扩散模型DDPM原理推导|AI绘画底层模型](https://www.bilibili.com/video/BV1p24y1K7Pf)
