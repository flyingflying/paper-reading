
# PLMEE

[TOC]

## 简介

论文的全名是: **Exploring Pre-trained Language Models for Event Extraction and Generation** , 其发表于 ACL2019。

本篇论文实现的任务有两个: **事件抽取** 和 **事件生成** (数据增强) 。本博客也按照这个思路来介绍这篇论文。

官方链接:

+ [论文地址](https://aclanthology.org/P19-1522/)
+ 代码地址: 截至 2023年3月21日, 没有找到其代码, 只找到了第三方复现的代码

## 事件抽取

本论文实现的事件抽取采用的是 pipeline 管道模型, 分为触发词检测和论元检测, 其中触发词检测使用的是序列标注的方式, 论元检测使用的是基于 span 的方式。

### 触发词检测

没什么好说的, 和 BIO 序列标注是一致的, 实体就是触发词, 实体的类型就是事件的类型, 使用的是 BERT + softmax 基础模型, 没有用 CRF, 属于 NER 基础任务。

### 论元检测

在 ACE2005 数据集中, 会出现 argument overlap 的问题, 即在一个事件中, 某些实体会承担多个 "角色"。比方说, 对于下面这句话:

```text
恐怖分子张三在商场中引爆了炸弹, 也把自己炸死了。
(The explosion killed the bomber and three shopper.)
```

对于 "引爆炸弹" 这个事件, "张三" 即使 "袭击者" (attacker), 也是 "受害者" (victim) 。

为了解决这一问题, 论文作者没有使用 BIO 序列标注的方式来识别论元, 而是使用基于 span 的方式, 模型的结构如下:

+ 使用 BERT 模型对句子进行编码, 触发词对应的 `token_type_ids` 为 1, 其余的为 0
+ 对于每一个论元角色, 我们设计两个二分类任务, 一个去预测实体的头 token, 一个去预测实体的尾 token, 也就是说, 如果有 $n$ 个论元角色, 那么就有 $2n$ 个二分类任务

这种基于 span 的预测方式和抽取式 QA 任务是一致的。论文作者还用 **状态机** 的方式描述了基于 span 的解码过程。但是其伪代码似乎有问题。

### 多任务 loss 权重设置

需要注意的是, 在论元检测中, 是将所有事件类型的论元角色放在一起识别的。

比方说, "Injure (受伤)" 和 "Attack (袭击)" 两个事件类型都有 "Victim (受害者)" 这个论元角色, 那么我们将其作为一个类型。

很多基于 pipeline 的事件抽取模型都是这么做的, 其优势是不同事件类型的论元角色可以交叉学习信息, 缺点是没有了事件类型的依赖 (可能存在 "受害者" 只属于 "受伤" / "袭击" 两个中一个的情况。)

此时, 在训练时, 不同事件类型的样本数量差别较大, 因此在计算 loss 时, 需要设置不同的 **权重** 。

作者提出了 RF-IEF, 是仿照 TF-IDF 来设计的:

+ RF(r, v): 论元角色 r 在事件类型 v 中出现的 **频率**
+ EF(r): 论元角色在 r 在所有事件中出现的 **频率**
+ IEF(r): EF(r) 的倒数取对数
+ RF-IEF(r, v) = RF(r, v) * IEF(r)

对于每一个事件类型, 我们用 softmax 函数将 RF-IEF 转换成最终的 loss 权重值。这样, 即使论元角色 r 没有出现在事件类型 v 中 (RF-IEF 的值为 0), softmax 函数也会给其分配一定的权重值。

按照作者的实验, 使用这种方式论元识别的 F1 分数值提升了 0.5% .

### 评测标准

一般情况下, 事件检测会从四个角度来看性能: 触发词检测, 触发词分类, 论元检测 和 论元分类。

检测任务仅仅是标识出来实体, 分类任务需要将实体和标签对应上。

本文的作者还指出, 之前的工作, 如果一个实体有多个论元角色, 其只要匹配成功其中一个, 就算正确。本文修改了检查方式, 对于这样的论元, 必须所有的角色都匹配才算正确。

这里我吐槽以一下, 为什么不直接以 事件 为单位计算 F1 分数呢? 这样比较不是更加公平吗?

## 事件生成 (数据增强)

### 相关工作

相较于 CV 的数据, NLP 领域的数据标注难度可以说是非常高。事件抽取的标注难度虽然达不到 成分/依存句法树, 但依旧是一件非常困难的事情。数据增强就是一件非常重要的事情了。论文作者将之间的工作分成三类:

+ 基于 distant supervision 假设
  + [Automatically labeled data generation for large scale event extraction](https://aclanthology.org/P17-1038/)
  + [Scale Up Event Extraction Learning via Automatic Training Data Generation](https://arxiv.org/abs/1712.03665)
+ 使用 Abstract Meaning Representation 进行扩展
  + [Liberal Event Extraction and Event Schema Induction](https://aclanthology.org/P16-1025/)
+ 使用 [FrameNet](https://framenet.icsi.berkeley.edu/) 数据集进行扩展
  + [Leveraging framenet to improve automatic event detection](https://aclanthology.org/P16-1201/)
  + [Exploiting argument information to improve event detection via supervised attention mechanisms](https://aclanthology.org/P17-1164/)

在这篇论文中, 作者尝试利用 **预训练模型**, 通过 **文本替换** 的方式进行数据增强。

### 论元替换

将一个句子中的论元用其它句子中同角色的论元进行替换, 替换后论元的角色保持不变。

根据我对论文的理解, 大致流程如下:

1. 将训练集中所有句子的 token 用 ELMO 模型进行编码, 生成 **token 向量**
2. 对于句子中的每一个论元, 取其对应的 token 向量, 然后取平均, 得到 **论元向量**
3. 将句子中的每一个论元向量和其它句子中同角色的论元向量之间计算 cosine 相似度, 然后进行排序, 取排名靠前的 10% 论元作为 **候选论元**

这样, 我们就可以得到大量的训练数据了。需要注意的有以下几点:

1. 如果某个论元在某个事件中承担了超过一个角色, 则忽略 (既不会对其进行替换, 也不会用其去替换其它论元)。
2. 在进行替换时, 一个论元有 80% 的概率会被替换, 替换词从候选论元中随机选择, 20% 的概率不会被替换
3. 触发词不会被替换

需要说明的是, 作者使用了 ELMO 模型对论元进行编码, 是基于以下考虑:

1. 论元向量的编码使用了上下文的信息, 这样比 word2vec 更加合理
2. ELMO 模型可以处理 OOV 问题 (这一点需要考证, BERT 模型是基于 WordPiece 分词的, 应该也是没问题的)

### 辅助词替换

我们将一句话中除去 论元 和 触发词的其它词称为 **辅助词** (Adjunct Token), 包括词语, 数字和标签符号。

如果只对论元进行替换, 那么模型很容易过拟合。怎么办呢? 那就是对辅助词进行替换。

根据我对论文的理解, 大致流程如下:

1. 使用训练集中的句子, 微调 BERT 的 MLM 任务, 得到一个新的模型
2. 将上一阶段 (论元替换) 得到的新句子, 15% 的 token 掩掉 (用 `[MASK]` 特殊字符进行替换), 注意触发词和论元部分不能掩掉
3. 将第二步得到的句子输入第一步训练得到的模型中, 用预测的 token 替换 `[MASK]` 处的 token, 得到新的句子

需要注意的是, 非 `[MASK]` 部分预测的 token 不会进行替换, 只会替换 `[MASK]` 部分的。

### 打分指标

通过前两阶段得到的新句子, 我们需要去评估其质量。作者认为应该从以下两个方面来评估:

+ 新句子的合理性
+ 和数据集中句子之间的差异性

根据上述想法, 作者提出两个评价指标: perplexity 和 distance 。

Perplexity, 困惑度, 是评价语言模型常用的指标。详细的定义可以参考 [Perplexity of fixed-length models](https://huggingface.co/docs/transformers/perplexity) 。

需要注意的是, 这里的定义是针对 CLM 模型而言的, 和主题模型中的困惑度不是一回事。对于 MLM 模型来说, 没有官方定义。BERT 论文中作者也没有详细说明其计算过程。

本文作者对于 perplexity 的定义是将第二阶段模型预测 `[MASK]` 部分的概率取平均值, 记作 **PPL** 。

从概率论的角度来说, 这样是不合理的, 因为概率一般是不能直接相加的, 需要取对数后才能相加。从语言学的角度来说, 也不合理, perplexity 的翻译是困惑度, 应该是越小越好, 根据作者的定义变成了越大越好了。

Distance 距离衡量的是新句子和原本训练集之间的距离。将新句子和原本训练集中的所有句子计算 cosine 距离, 然后取平均值, 得到新句子和原本训练集之间的距离, 记作 **DIS** 。

PPL 和 DIS 取值都在 `[0, 1]` 之间, 都是越大越好。最后引入 $\lambda$ 平衡两个指标, 公式如下:

$$
Q = 1 - (\lambda \cdot PPL + (1 - \lambda) \cdot DIS)
$$

实验部分说, $\lambda = 0.5$ 最好, 没有说 $Q$ 的值, 根据示例图, 猜测应该是 0.5 。

### 优势和缺陷

这里的数据增加方式可以说是非常巧妙。用预训练模型来计算词语之间地相似性, 比直接用近义词词典的方式要好很多, 除此之外, 用 MLM 任务来替换词语也是非常好的想法。

根据作者后续的实验, 使用数据增强后, 性能提升了 0.4% - 0.9% 。虽然想法很好, 但是性能提升并不明显。

作者也说明了问题。主要在辅助词替换部分, 辅助词替换后会改变实体的 "主被动" 关系。比方说 "drive to" 可能会被替换成 "return from" , 那么实体就从 "目的地" 变成了 "出发地"。

## 总结

本文实现的事件抽取方案可以说非常中规中矩, 采用 pipeline 的方式, 触发词检测用 BIO 序列标注, 论元检测用 span NER 的方式, 数据不够就进行数据增强。

数据增强的方式比较巧妙: 用预训练模型生成的实体向量计算 cosine 相似度, 产生候选实体; 用 MLM 任务替换部分 token。
