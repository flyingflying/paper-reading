
# Flash Attention

[TOC]

## 引用

论文或者官方博客:

+ [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
+ [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
+ [Flash-Decoding for long-context inference](https://crfm.stanford.edu/2023/10/12/flashdecoding.html)
+ [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
+ [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)

官方代码:

+ [flash-attention](https://github.com/Dao-AILab/flash-attention)
+ [vLLM](https://github.com/vllm-project/vllm)
+ [streaming-llm](https://github.com/mit-han-lab/streaming-llm)

优质内容:

+ [FlashAttention 的速度优化原理是怎样的？](https://www.zhihu.com/question/611236756)
+ [FlashAttention-PyTorch](https://github.com/shreyansh26/FlashAttention-PyTorch)

nvcc: C/C++ 编译器, 支持 cuda 代码
